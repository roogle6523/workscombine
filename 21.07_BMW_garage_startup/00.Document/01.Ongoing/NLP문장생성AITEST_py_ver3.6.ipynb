{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import os\r\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "print(sys.version)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "import re\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "!pip install torch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from torch) (3.10.0.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "!pip install MXNet"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: MXNet in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (1.7.0.post2)\n",
      "Collecting numpy<1.17.0,>=1.8.2\n",
      "  Using cached numpy-1.16.6-cp36-cp36m-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from MXNet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from MXNet) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->MXNet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->MXNet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->MXNet) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->MXNet) (2021.5.30)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "Successfully installed numpy-1.16.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.9.1 requires numpy>=1.17, but you have numpy 1.16.6 which is incompatible.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "!pip3 install --upgrade mxnet gluonnlp"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mxnet in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (1.7.0.post2)\n",
      "Collecting gluonnlp\n",
      "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "Requirement already satisfied: numpy<1.17.0,>=1.8.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (1.16.6)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: cython in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from gluonnlp) (0.29.24)\n",
      "Requirement already satisfied: packaging in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from gluonnlp) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py): started\n",
      "  Building wheel for gluonnlp (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for gluonnlp\n",
      "Failed to build gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "    Running setup.py install for gluonnlp: started\n",
      "    Running setup.py install for gluonnlp: finished with status 'error'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\com\\anaconda3\\envs\\py36_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\com\\AppData\\Local\\Temp\\pip-wheel-2kc3a3jh'\n",
      "       cwd: C:\\Users\\com\\AppData\\Local\\Temp\\pip-install-w22sg0bz\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\n",
      "  Complete output (123 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.6\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\n",
      "  copying src\\gluonnlp\\base.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "  copying src\\gluonnlp\\_constants.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "  copying src\\gluonnlp\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "  copying src\\gluonnlp\\calibration\\collector.py -> build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "  copying src\\gluonnlp\\calibration\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\baidu_ernie_data.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\candidate_sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\classification.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\conll.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\dataloader.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\dataset.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\datasetloader.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\intent_slot.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\question_answering.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\registry.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\sentiment.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\stream.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\super_glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\transforms.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\translation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\utils.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\word_embedding_evaluation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\evaluation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\token_embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "  copying src\\gluonnlp\\initializer\\initializer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "  copying src\\gluonnlp\\initializer\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\activation_regularizer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\label_smoothing.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\loss.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\length_normalized_loss.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\masked_accuracy.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\attention_cell.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\bert.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\bilm_encoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\block.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\convolutional_encoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\elmo.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\highway.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\info.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\lstmpcellwithclip.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\parameter.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\sampled_block.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\seq2seq_encoder_decoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\sequence_sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\transformer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\translation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\utils.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "  copying src\\gluonnlp\\optimizer\\bert_adam.py -> build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "  copying src\\gluonnlp\\optimizer\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\files.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\parallel.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\parameter.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\seed.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\version.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\bert.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\elmo.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\subwords.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\vocab.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\batchify.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "  copying src\\gluonnlp\\data\\bert\\glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "  copying src\\gluonnlp\\data\\bert\\squad.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "  copying src\\gluonnlp\\data\\bert\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\google_billion_word.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\large_text_compression_benchmark.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\wikitext.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "  copying src\\gluonnlp\\data\\xlnet\\squad.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "  copying src\\gluonnlp\\data\\xlnet\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "  creating build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\cache.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "  running egg_info\n",
      "  writing src\\gluonnlp.egg-info\\PKG-INFO\n",
      "  writing dependency_links to src\\gluonnlp.egg-info\\dependency_links.txt\n",
      "  writing requirements to src\\gluonnlp.egg-info\\requires.txt\n",
      "  writing top-level names to src\\gluonnlp.egg-info\\top_level.txt\n",
      "  reading manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '*.py' under directory 'gluonnlp'\n",
      "  warning: no previously-included files matching '*' found under directory 'tests'\n",
      "  warning: no previously-included files matching '*' found under directory 'scripts'\n",
      "  writing manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "  copying src\\gluonnlp\\data\\fast_bert_tokenizer.c -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\fast_bert_tokenizer.pyx -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "  running build_ext\n",
      "  skipping 'src/gluonnlp/data\\fast_bert_tokenizer.c' Cython extension (up-to-date)\n",
      "  building 'gluonnlp.data.fast_bert_tokenizer' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for gluonnlp\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\com\\anaconda3\\envs\\py36_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\com\\AppData\\Local\\Temp\\pip-record-e3pec7wq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\com\\anaconda3\\envs\\py36_env\\Include\\gluonnlp'\n",
      "         cwd: C:\\Users\\com\\AppData\\Local\\Temp\\pip-install-w22sg0bz\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\n",
      "    Complete output (123 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.6\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\n",
      "    copying src\\gluonnlp\\base.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "    copying src\\gluonnlp\\_constants.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "    copying src\\gluonnlp\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "    copying src\\gluonnlp\\calibration\\collector.py -> build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "    copying src\\gluonnlp\\calibration\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\calibration\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\baidu_ernie_data.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\candidate_sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\classification.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\conll.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\dataloader.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\dataset.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\datasetloader.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\intent_slot.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\question_answering.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\registry.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\sentiment.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\stream.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\super_glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\transforms.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\translation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\utils.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\word_embedding_evaluation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "    copying src\\gluonnlp\\embedding\\evaluation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "    copying src\\gluonnlp\\embedding\\token_embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "    copying src\\gluonnlp\\embedding\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\embedding\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "    copying src\\gluonnlp\\initializer\\initializer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "    copying src\\gluonnlp\\initializer\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\initializer\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "    copying src\\gluonnlp\\loss\\activation_regularizer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "    copying src\\gluonnlp\\loss\\label_smoothing.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "    copying src\\gluonnlp\\loss\\loss.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "    copying src\\gluonnlp\\loss\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\loss\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "    copying src\\gluonnlp\\metric\\length_normalized_loss.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "    copying src\\gluonnlp\\metric\\masked_accuracy.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "    copying src\\gluonnlp\\metric\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\metric\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\attention_cell.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\bert.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\bilm_encoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\block.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\convolutional_encoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\elmo.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\highway.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\info.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\lstmpcellwithclip.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\parameter.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\sampled_block.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\seq2seq_encoder_decoder.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\sequence_sampler.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\transformer.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\translation.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\utils.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    copying src\\gluonnlp\\model\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "    copying src\\gluonnlp\\optimizer\\bert_adam.py -> build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "    copying src\\gluonnlp\\optimizer\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\optimizer\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\files.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\parallel.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\parameter.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\seed.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\version.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    copying src\\gluonnlp\\utils\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\utils\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    copying src\\gluonnlp\\vocab\\bert.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    copying src\\gluonnlp\\vocab\\elmo.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    copying src\\gluonnlp\\vocab\\subwords.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    copying src\\gluonnlp\\vocab\\vocab.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    copying src\\gluonnlp\\vocab\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\vocab\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "    copying src\\gluonnlp\\data\\batchify\\batchify.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "    copying src\\gluonnlp\\data\\batchify\\embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "    copying src\\gluonnlp\\data\\batchify\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "    copying src\\gluonnlp\\data\\batchify\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\batchify\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "    copying src\\gluonnlp\\data\\bert\\glue.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "    copying src\\gluonnlp\\data\\bert\\squad.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "    copying src\\gluonnlp\\data\\bert\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\bert\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "    copying src\\gluonnlp\\data\\corpora\\google_billion_word.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "    copying src\\gluonnlp\\data\\corpora\\large_text_compression_benchmark.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "    copying src\\gluonnlp\\data\\corpora\\wikitext.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "    copying src\\gluonnlp\\data\\corpora\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\corpora\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "    copying src\\gluonnlp\\data\\xlnet\\squad.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "    copying src\\gluonnlp\\data\\xlnet\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\data\\xlnet\n",
      "    creating build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "    copying src\\gluonnlp\\model\\train\\cache.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "    copying src\\gluonnlp\\model\\train\\embedding.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "    copying src\\gluonnlp\\model\\train\\language_model.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "    copying src\\gluonnlp\\model\\train\\__init__.py -> build\\lib.win-amd64-3.6\\gluonnlp\\model\\train\n",
      "    running egg_info\n",
      "    writing src\\gluonnlp.egg-info\\PKG-INFO\n",
      "    writing dependency_links to src\\gluonnlp.egg-info\\dependency_links.txt\n",
      "    writing requirements to src\\gluonnlp.egg-info\\requires.txt\n",
      "    writing top-level names to src\\gluonnlp.egg-info\\top_level.txt\n",
      "    reading manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no files found matching '*.py' under directory 'gluonnlp'\n",
      "    warning: no previously-included files matching '*' found under directory 'tests'\n",
      "    warning: no previously-included files matching '*' found under directory 'scripts'\n",
      "    writing manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "    copying src\\gluonnlp\\data\\fast_bert_tokenizer.c -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    copying src\\gluonnlp\\data\\fast_bert_tokenizer.pyx -> build\\lib.win-amd64-3.6\\gluonnlp\\data\n",
      "    running build_ext\n",
      "    skipping 'src/gluonnlp/data\\fast_bert_tokenizer.c' Cython extension (up-to-date)\n",
      "    building 'gluonnlp.data.fast_bert_tokenizer' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\com\\anaconda3\\envs\\py36_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\com\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w22sg0bz\\\\gluonnlp_367c5bf97f2344abafb3d8e16c604663\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\com\\AppData\\Local\\Temp\\pip-record-e3pec7wq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\com\\anaconda3\\envs\\py36_env\\Include\\gluonnlp' Check the logs for full command output.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "!pip install python3-dev"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python3-dev (from versions: none)\n",
      "ERROR: No matching distribution found for python3-dev\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "pip install --upgrade mxnet>=1.6.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install sentencepiece "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install transformers "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from transformers) (2.18.4)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from transformers) (0.8)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from transformers) (21.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-5.4.1-cp36-cp36m-win_amd64.whl (209 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp36-cp36m-win_amd64.whl (2.0 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2021.7.6-cp36-cp36m-win_amd64.whl (270 kB)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-4.6.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests->transformers) (1.22)\n",
      "Collecting click\n",
      "  Using cached click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: six in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: zipp, importlib-metadata, tqdm, regex, joblib, filelock, click, tokenizers, sacremoses, pyyaml, numpy, huggingface-hub, transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.6\n",
      "    Uninstalling numpy-1.16.6:\n",
      "      Successfully uninstalled numpy-1.16.6\n",
      "Successfully installed click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.12 importlib-metadata-4.6.1 joblib-1.0.1 numpy-1.19.5 pyyaml-5.4.1 regex-2021.7.6 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.61.2 transformers-4.9.1 zipp-3.5.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mxnet 1.7.0.post2 requires numpy<1.17.0,>=1.8.2, but you have numpy 1.19.5 which is incompatible.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\r\n",
    "from gluonnlp.data import SentencepieceTokenizer\r\n",
    "from kogpt2.utils import get_tokenizer\r\n",
    "\r\n",
    "tok_path = get_tokenizer()\r\n",
    "model, vocab = get_pytorch_kogpt2_model()\r\n",
    "tok = SentencepieceTokenizer(tok_path)\r\n",
    "sent = '2019ÎÖÑ ÌïúÌï¥Î•º Î≥¥ÎÇ¥Î©∞,'\r\n",
    "toked = tok(sent)\r\n",
    "while 1:\r\n",
    "  input_ids = torch.tensor([vocab[vocab.bos_token],]  + vocab[toked]).unsqueeze(0)\r\n",
    "  pred = model(input_ids)[0]\r\n",
    "  gen = vocab.to_tokens(torch.argmax(pred, axis=-1).squeeze().tolist())[-1]\r\n",
    "  if gen == '</s>':\r\n",
    "      break\r\n",
    "  sent += gen.replace('‚ñÅ', ' ')\r\n",
    "  toked = tok(sent)\r\n",
    "sent"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kogpt2'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3d746071d398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkogpt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpytorch_kogpt2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_pytorch_kogpt2_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgluonnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentencepieceTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkogpt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kogpt2'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gluonnlp as nlp"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gluonnlp'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e68c3b59001c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgluonnlp\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gluonnlp'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install mxnet"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mxnet in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (1.7.0.post2)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: numpy<1.17.0,>=1.8.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (1.16.6)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2021.5.30)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\com\\anaconda3\\envs\\py36_env\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mxnet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\r\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\r\n",
    "  pad_token='<pad>', mask_token='<mask>') \r\n",
    "tokenizer.tokenize(\"ÏïàÎÖïÌïòÏÑ∏Ïöî. ÌïúÍµ≠Ïñ¥ GPT-2 ÏûÖÎãàÎã§.üò§:)l^o\")\r\n",
    "['‚ñÅÏïàÎÖï', 'Ìïò', 'ÏÑ∏', 'Ïöî.', '‚ñÅÌïúÍµ≠Ïñ¥', '‚ñÅG', 'P', 'T', '-2', '‚ñÅÏûÖ', 'ÎãàÎã§.', 'üò§', ':)', 'l^o']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.83M/2.83M [00:00<00:00, 3.12MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['‚ñÅÏïàÎÖï',\n",
       " 'Ìïò',\n",
       " 'ÏÑ∏',\n",
       " 'Ïöî.',\n",
       " '‚ñÅÌïúÍµ≠Ïñ¥',\n",
       " '‚ñÅG',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '‚ñÅÏûÖ',\n",
       " 'ÎãàÎã§.',\n",
       " 'üò§',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "import torch\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "num = 41\r\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\r\n",
    "text = 'ÌîºÎ∂ÄÏùò Ï°∞ÏßÅÏù¥ Í¥¥ÏÇ¨ÎêúÎã§Î©¥'\r\n",
    "input_ids = tokenizer.encode(text)\r\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\r\n",
    "                           max_length = int(\"{}\".format(num)),\r\n",
    "                           repetition_penalty=1.9,\r\n",
    "                           pad_token_id=tokenizer.pad_token_id,\r\n",
    "                           eos_token_id=tokenizer.eos_token_id,\r\n",
    "                           bos_token_id=tokenizer.bos_token_id,\r\n",
    "                           use_cache=True)\r\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\r\n",
    "\r\n",
    "print(generated)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ÌîºÎ∂ÄÏùò Ï°∞ÏßÅÏù¥ Í¥¥ÏÇ¨ÎêúÎã§Î©¥ Í∑∏ ÏõêÏù∏ÏùÄ ÏïÑÏßÅ Î∞ùÌòÄÏßÄÏßÄ ÏïäÏïòÎã§.\n",
      "Í∑∏Îü¨ÎÇò Ïù¥ Í∞ôÏùÄ ÏÇ¨Ïã§ÏùÄ Ïù¥ÎØ∏ Ïó¨Îü¨ Ï∞®Î°Ä Î∞ùÌòÄÏ°åÏóàÎã§.\n",
      "Ïù¥Î≤à Ïó∞Íµ¨ÏóêÏÑúÎäî ÎáåÏ°∏Ï§ë ÌôòÏûêÏùò ÏïΩ 80%Í∞Ä ÏïåÏ∏†ÌïòÏù¥Î®∏Î≥ëÏù¥ÎùºÎäî ÏÇ¨Ïã§ÏùÑ Î∞ùÌòÄÎÉàÎã§.\n",
      "Ïïå\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "generated[-2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "temperature = 202\r\n",
    "measure = 'Fahrenheit'\r\n",
    "print('Water boils at %d degrees %s' % (temperature, measure))     # 1\r\n",
    "print('Water boils at {} degrees {}'.format(num, measure)) # 2\r\n",
    "print(f'Water boils at {temperature} degrees {measure}') "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Water boils at 202 degrees Fahrenheit\n",
      "Water boils at 30 degrees Fahrenheit\n",
      "Water boils at 202 degrees Fahrenheit\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "import torch\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "num = 300\r\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\r\n",
    "text = 'ÌîºÎ∂ÄÏùò Ï°∞ÏßÅÏù¥ Í¥¥ÏÇ¨ÎêúÎã§Î©¥'\r\n",
    "input_ids = tokenizer.encode(text)\r\n",
    "while True:\r\n",
    "    gen_ids = model.generate(torch.tensor([input_ids]),\r\n",
    "                            max_length = int(\"{}\".format(num)),\r\n",
    "                            repetition_penalty=1.9,\r\n",
    "                            pad_token_id=tokenizer.pad_token_id,\r\n",
    "                            eos_token_id=tokenizer.eos_token_id,\r\n",
    "                            bos_token_id=tokenizer.bos_token_id,\r\n",
    "                            use_cache=True)\r\n",
    "    generated = tokenizer.decode(gen_ids[0,:].tolist())\r\n",
    "    num += 1\r\n",
    "    print(\"Ï°∞Ï†ïÏ§ë.\")\r\n",
    "    if generated[-1] == \"\\n\":\r\n",
    "        print(generated)\r\n",
    "        break\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "ÌîºÎ∂ÄÏùò Ï°∞ÏßÅÏù¥ Í¥¥ÏÇ¨ÎêúÎã§Î©¥ Í∑∏ ÏõêÏù∏ÏùÄ ÏïÑÏßÅ Î∞ùÌòÄÏßÄÏßÄ ÏïäÏïòÎã§.\n",
      "Í∑∏Îü¨ÎÇò Ïù¥ Í∞ôÏùÄ ÏÇ¨Ïã§ÏùÄ Ïù¥ÎØ∏ Ïó¨Îü¨ Ï∞®Î°Ä Î∞ùÌòÄÏ°åÏóàÎã§.\n",
      "Ïù¥Î≤à Ïó∞Íµ¨ÏóêÏÑúÎäî ÎáåÏ°∏Ï§ë ÌôòÏûêÏùò ÏïΩ 80%Í∞Ä ÏïåÏ∏†ÌïòÏù¥Î®∏Î≥ëÏù¥ÎùºÎäî ÏÇ¨Ïã§ÏùÑ Î∞ùÌòÄÎÉàÎã§.\n",
      "ÏïåÏ∏†Îäî Ïã†Í≤ΩÏ†ÑÎã¨Î¨ºÏßàÏù∏ ÏïÑÏÑ∏Ìã∏ÏΩúÎ¶∞Ïùò Î∂ÑÎπÑÎ•º Ï¥âÏßÑÌï¥ ÎáåÏùò ÌòàÎ•òÎüâÏùÑ Ï¶ùÍ∞ÄÏãúÌÇ®Îã§.\n",
      "ÎòêÌïú ÌòàÍ¥Ä ÎÇ¥ÌîºÏÑ∏Ìè¨Ïùò Í∏∞Îä•ÏùÑ ÌôúÏÑ±ÌôîÏãúÏºú ÎèôÎß•Í≤ΩÌôîÎ•º ÏòàÎ∞©ÌïúÎã§.\n",
      "ÎáåÌòàÍ¥ÄÏù¥ Ï¢ÅÏïÑÏßÄÎ©¥ ÌòàÏï°ÏàúÌôòÏóê Ïû•Ïï†Í∞Ä ÏÉùÍ≤® Ïã¨ÌòàÍ¥ÄÏßàÌôòÏùÑ Ïú†Î∞úÌï† Ïàò ÏûàÎã§.\n",
      "Îî∞ÎùºÏÑú Ïù¥Î≤à Ïó∞Íµ¨Îäî Ïã¨Ïû•ÎßàÎπÑÎÇò ÎãπÎá®Î≥ë, Í≥†ÌòàÏïï, Í≥†ÏßÄÌòà, ÎπÑÎßå, Ìù°Ïó∞, ÏùåÏ£º, Ìù°Ïó∞ Îì± Îã§ÏñëÌïú ÏúÑÌóòÏöîÏù∏ÏùÑ Í∞ÄÏßÑ ÌôòÏûêÎì§ÏóêÍ≤å ÎèÑÏõÄÏù¥ Îê† Í≤ÉÏúºÎ°ú Í∏∞ÎåÄÎêúÎã§.\n",
      "Ïó∞Íµ¨ÌåÄÏùÄ ‚ÄúÏã¨Ïû•ÎßàÎπÑ ÌôòÏûêÎäî ÎåÄÎ∂ÄÎ∂Ñ ÏπòÎß§ÎÇò ÌååÌÇ®Ïä®Î≥ëÏùÑ ÏïìÍ≥† ÏûàÎäî Í≤ΩÏö∞Í∞Ä ÎßéÎã§‚ÄùÎ©∞ ‚ÄúÏù¥Îì§ÏùÄ Îã§Î•∏ ÏßàÌôòÍ≥º ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú Ï°∞Í∏∞ Î∞úÍ≤¨ Î∞è ÏπòÎ£åÍ∞Ä Ï§ëÏöîÌïòÎã§‚ÄùÍ≥† ÎßêÌñàÎã§.\n",
      "Ïù¥Ïñ¥ Í∑∏Îäî ‚ÄúÏïÑÏßÅÍπåÏßÄ Ï†ïÌôïÌïú ÏõêÏù∏ÏùÑ Í∑úÎ™ÖÌïòÏßÄ Î™ªÌïú ÏÉÅÌÉú‚ÄùÎùºÎ©∞ ‚ÄúÍ∑∏ÎèôÏïàÏùò Ïó∞Íµ¨Î•º ÌÜµÌï¥ Î∞ùÌòÄÏßÑ Î∞îÎ°úÎäî Ïã¨Í∑º Í≤ΩÏÉâÏù¥ÎÇò Ïã¨Î∂ÄÏ†Ñ, ÌòëÏ∞©Ï¶ù, ÎßêÏ¥àÎèôÎß•Í≤ΩÌôî Îì±Ïùò Ìï©Î≥ëÏ¶ùÏù¥ Î∞úÏÉùÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÍ∏∞ ÎïåÎ¨∏Ïóê Ï°∞Í∏∞Ïóê Ï†ÅÏ†àÌïú ÏπòÎ£åÎ•º Î∞õÎäî Í≤ÉÏù¥ Ï¢ãÎã§‚ÄùÎùºÍ≥† ÎçßÎ∂ôÏòÄÎã§.</d> ÏßÄÎÇúÎã¨ 30Ïùº Ïò§ÌõÑ ÏÑúÏö∏ Ï¢ÖÎ°úÍµ¨ ÏÑ∏Ï¢ÖÎ°ú Ï†ïÎ∂ÄÏ§ëÏïôÏ≤≠ÏÇ¨ÏóêÏÑú Ïó¥Î¶∞ ‚Äò2010 ÎåÄÌïúÎØºÍµ≠ ÏÇ¨ÌöåÍ≥µÌóåÎåÄÏÉÅ‚Äô ÏãúÏÉÅÏãùÏóêÏÑú ÌïúÍµ≠ÏÇ¨ÌöåÎ≥µÏßÄÌòëÏùòÌöå(ÌöåÏû• Î∞ïÏÇºÍµ¨)Îäî Î≥¥Í±¥Î≥µÏßÄÎ∂Ä Ïû•Í¥ÄÏÉÅÏùÑ ÏàòÏÉÅÌñàÎã§.\n",
      "ÌïúÍµ≠ÏÇ¨ÌöåÎ¥âÏÇ¨Îã®ÏùÄ ÏÇ¨ÌöåÎ≥µÏßÄ Î∂ÑÏïº Í≥µÏùµÏÇ¨ÏóÖÏùÑ Î∞úÍµ¥ÌïòÍ≥† ÏßÄÏõêÌïòÎäî ÎπÑÏòÅÎ¶¨ ÎØºÍ∞ÑÎã®Ï≤¥Îã§.\n",
      "ÏßÄÎÇú 2005ÎÖÑÎ∂ÄÌÑ∞ Îß§ÎÖÑ Ï†ÑÍµ≠ ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥ÏôÄ Ìï®Íªò ÏßÄÏó≠ÏÇ¨Ìöå ÏÜåÏô∏Í≥ÑÏ∏µÏùÑ ÏúÑÌïú Î¥âÏÇ¨ÌôúÎèôÏùÑ ÌéºÏπòÍ≥† ÏûàÏúºÎ©∞, Ïò¨Ìï¥Îäî Ï¥ù 4ÏñµÏõêÏùò Í∏∞Í∏àÏùÑ Ï°∞ÏÑ±ÌñàÎã§.\n",
      "Î∞ï ÌöåÏû•ÏùÄ ÏàòÏÉÅ ÏÜåÍ∞êÏùÑ Î¨ªÎäî Í∏∞ÏûêÎì§Ïùò ÏßàÎ¨∏Ïóê ÎåÄÌï¥ ‚ÄúÏö∞Î¶¨ ÏÇ¨ÌöåÍ∞Ä Ïñ¥Î†§Ïö¥ Ïù¥ÏõÉÎì§ÏóêÍ≤å Ìù¨ÎßùÏùÑ Ï£ºÎäî Îî∞ÎúªÌïú ÏÇ¨ÌöåÎ•º ÎßåÎì§Í∏∞ ÏúÑÌï¥ ÎÖ∏Î†•ÌïòÍ≤†Îã§.‚ÄùÍ≥† Î∞ùÌòîÎã§.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "import torch\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "num = 150\r\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\r\n",
    "text = 'ÏõêÌîºÏä§, Í∞ÄÌûàÎ©ÄÌã∞Î∞§, Ï∞ΩÎ¨∏ÌòïÏóêÏñ¥Ïª®, ÏóêÏñ¥Ïª®Ïã§Ïô∏Í∏∞Ïª§Î≤Ñ, Ìè¨ÏºìÎ™¨Ïπ¥Îìú '\r\n",
    "input_ids = tokenizer.encode(text)\r\n",
    "while True:\r\n",
    "    gen_ids = model.generate(torch.tensor([input_ids]),\r\n",
    "                            max_length = int(\"{}\".format(num)),\r\n",
    "                            repetition_penalty=1.9,\r\n",
    "                            pad_token_id=tokenizer.pad_token_id,\r\n",
    "                            eos_token_id=tokenizer.eos_token_id,\r\n",
    "                            bos_token_id=tokenizer.bos_token_id,\r\n",
    "                            use_cache=True)\r\n",
    "    generated = tokenizer.decode(gen_ids[0,:].tolist())\r\n",
    "    num += 1\r\n",
    "    print(\"Ï°∞Ï†ïÏ§ë.\")\r\n",
    "    if generated[-1] == \"\\n\":\r\n",
    "        print(generated)\r\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "Ï°∞Ï†ïÏ§ë.\n",
      "ÏõêÌîºÏä§, Í∞ÄÌûàÎ©ÄÌã∞Î∞§, Ï∞ΩÎ¨∏ÌòïÏóêÏñ¥Ïª®, ÏóêÏñ¥Ïª®Ïã§Ïô∏Í∏∞Ïª§Î≤Ñ, Ìè¨ÏºìÎ™¨Ïπ¥Îìú ÍπåÏßÄ.\n",
      "Ïù¥Îü∞Ï†ÄÎ†áÍ≤å Îã§ÏñëÌïú Ï†úÌíàÎì§Ïù¥ Ï∂úÏãúÎêòÍ≥† ÏûàÏßÄÎßå Ï†ïÏûë ÏÜåÎπÑÏûêÎì§ÏùÄ Ïù¥Îü∞ Ï†úÌíàÏùÑ Íµ¨Îß§ÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞Í∞Ä ÎßéÎã§.\n",
      "ÌäπÌûàÎÇò ÏöîÏ¶ò Í∞ôÏùÄ ÎÇ†Ïî®ÏóêÎäî Ïã§ÎÇ¥ÏóêÏÑú ÏÉùÌôúÌïòÎäî ÏãúÍ∞ÑÏù¥ ÎßéÏïÑÏßÄÎ©¥ÏÑú Ïù¥Îü¨Ìïú Ï†úÌíàÏù¥ ÎçîÏö± Ïù∏Í∏∞Î•º ÎÅåÍ≥† ÏûàÎã§.\n",
      "Í∑∏Î†áÎã§Î©¥ Ïñ¥Îñ§ Í≤ÉÏù¥ Îçî Ï¢ãÏùÄÍ∞Ä!\n",
      "Î∞îÎ°ú 'ÏóêÏΩîÎ∞±'Ïù¥Îã§.\n",
      "'ÏïÑÏù¥Ïä§Î∞ïÏä§'Îäî ÏóêÏΩîÏùò Î∞±ÏùÑ Î™®Ìã∞Î∏åÎ°ú Ìïú ÎîîÏûêÏù∏ÏúºÎ°ú Ïã§Ïö©ÏÑ±Í≥º Í∏∞Îä•ÏÑ±ÏùÑ Î™®Îëê Í∞ñÏ∂ò ÏïÑÏù¥ÌÖúÏù¥Îã§.\n",
      "ÎòêÌïú Ïù¥ Ï†úÌíàÏùÄ Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ÏôÄ Ìï®Íªò ÏÇ¨Ïö©ÌïòÎ©¥ Î≥¥Îã§ ÏæåÏ†ÅÌïú Í≥µÍ∏∞Î•º ÎäêÎÇÑ Ïàò ÏûàÏñ¥ Ïó¨Î¶ÑÏ≤† ÌïÑÏàòÌíàÏúºÎ°ú ÏûêÎ¶¨ Ïû°ÏïòÎã§.\n",
      "ÏµúÍ∑º Îì§Ïñ¥ ÎØ∏ÏÑ∏Î®ºÏßÄ ÎïåÎ¨∏Ïóê Ïô∏Ï∂úÌïòÍ∏∞ Í∫ºÎ†§ÏßÄÎäî ÏÇ¨ÎûåÎì§Ïù¥ ÎäòÏñ¥ÎÇòÎ©¥ÏÑú ÏßëÏóêÏÑú ÏÜêÏâΩÍ≤å ÏÇ¨Ïö©Ìï†Ïàò ÏûàÎäî ÌôàÏºÄÏñ¥Î•º ÏúÑÌïú ÏàòÏöîÍ∞Ä ÎäòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('py36_env': conda)"
  },
  "interpreter": {
   "hash": "70f5aa6827e2eaf69aee478d99e62778054691c2f2c3adc8ab20a2da736a645c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}